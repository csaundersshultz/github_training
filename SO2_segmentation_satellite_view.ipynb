{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csaundersshultz/github_training/blob/main/SO2_segmentation_satellite_view.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bca93b6e-f5a4-4393-9a3e-b2c8f9efcc0f",
      "metadata": {
        "id": "bca93b6e-f5a4-4393-9a3e-b2c8f9efcc0f"
      },
      "outputs": [],
      "source": [
        "#SO2 segmentation this one will be focused on the satellite view case\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from datetime import datetime\n",
        "import math\n",
        "import time\n",
        "\n",
        "from skimage.segmentation import find_boundaries\n",
        "from skimage.restoration import denoise_wavelet, denoise_tv_chambolle, denoise_bilateral\n",
        "from skimage.exposure import rescale_intensity\n",
        "from skimage.measure import find_contours, approximate_polygon\n",
        "from skimage.draw import polygon\n",
        "\n",
        "import albumentations as A\n",
        "\n",
        "import tensorflow as tf\n",
        "layers = tf.keras.layers\n",
        "Model = tf.keras.Model\n",
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import MeanIoU, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle as sklearn_shuffle\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5524ee7d-c81c-48ef-8643-16acbd81e522",
      "metadata": {
        "id": "5524ee7d-c81c-48ef-8643-16acbd81e522"
      },
      "outputs": [],
      "source": [
        "#read in list of files, plot quick statistics and split into train/validation/test\n",
        "#set up generator to create masks and open files\n",
        "#initialize pre_fetching with generator and tf.data.Dataset\n",
        "#set up model architecture to accept 352x352 images\n",
        "#compile model, set up metrics and optimizer etc\n",
        "#train model\n",
        "#visualize training results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/csaundersshultz/github_training"
      ],
      "metadata": {
        "id": "EmO4HUuETeqt",
        "outputId": "4c3fa22a-7c31-486d-d3c7-4890c5ceb41f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EmO4HUuETeqt",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'github_training'...\n",
            "remote: Enumerating objects: 534, done.\u001b[K\n",
            "remote: Total 534 (delta 0), reused 0 (delta 0), pack-reused 534\u001b[K\n",
            "Receiving objects: 100% (534/534), 1.87 GiB | 25.28 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Updating files: 100% (537/537), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e0eb8311-fb55-4f3c-9a3c-c8dbd384197c",
      "metadata": {
        "id": "e0eb8311-fb55-4f3c-9a3c-c8dbd384197c",
        "outputId": "dbbb8d7e-c216-46af-b38d-497f33e1be24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167 / 350\n",
            "38 / 75\n",
            "31 / 75\n"
          ]
        }
      ],
      "source": [
        "data_list = pd.read_csv(\"github_training/final_500_training.csv\")\n",
        "#print(data_list.head())\n",
        "\n",
        "fn_train, fn_valtest, verts_train, verts_valtest = train_test_split(data_list.file, data_list.vertices, test_size=0.3, random_state=42)\n",
        "fn_val, fn_test, verts_val, verts_test = train_test_split(fn_valtest, verts_valtest, test_size=0.5, random_state=42)\n",
        "\n",
        "for dataset in (verts_train, verts_val, verts_test):\n",
        "    print( f\"{len(dataset.loc[dataset=='[]'])} / {len(dataset)}\" )\n",
        "#later, remove files in training/ which are not in the data_list. and find the one file not removed somehow. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29005e81-dad2-4dad-9dae-74da772ac5a8",
      "metadata": {
        "id": "29005e81-dad2-4dad-9dae-74da772ac5a8",
        "outputId": "29892d70-9e49-4848-b4a8-f98a06a3540b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator initiated with 350 sample\n",
            "Generator initiated with 75 sample\n",
            "Generator initiated with 75 sample\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-18 10:22:11.985463: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "def add_border(mask, bordersize=3):\n",
        "    if bordersize==0:\n",
        "        return mask\n",
        "    c = bordersize%2+1 #1 or 2, sets up alternating whether or not corners are counted, creates the most rounded shape\n",
        "    boundary = find_boundaries(mask, connectivity=c, mode='outer', background=0)\n",
        "    bounds_and_mask = np.logical_or(boundary, mask)\n",
        "    if bordersize!=0:#continue until bordersize is less than 1\n",
        "        extended_border = add_border(bounds_and_mask, bordersize=bordersize-1)\n",
        "        extended_border = (extended_border>0).astype('int')\n",
        "        bordered_mask = extended_border + mask\n",
        "        return bordered_mask #background=0, adjacents=1, hotspot=2\n",
        "    else: \n",
        "        final_mask = bounds_and_mask+mask \n",
        "        return final_mask\n",
        "\n",
        "def build_mask(vertices, shape=(352,352), bordersize=0):\n",
        "    \"\"\"\n",
        "    Takes a list of polygons with vertices. example:\n",
        "        [  [(1, 1), (1, 5), (5, 5), (5, 1)], #polygon1\n",
        "         [(10, 10), (10, 15), (15, 15), (15, 10)] ]  #polygon 2\n",
        "    \"\"\"\n",
        "    mask = np.zeros(shape, dtype=bool)\n",
        "    for polygon_vertices in vertices:\n",
        "        rr, cc = polygon(np.array(polygon_vertices)[:, 0], np.array(polygon_vertices)[:, 1], shape=shape)\n",
        "        mask[cc, rr] = True\n",
        "    \n",
        "    if bordersize>0:\n",
        "        mask = add_border(mask)\n",
        "    return mask\n",
        "\n",
        "def get_weights(mask):\n",
        "    weights = np.zeros(mask.shape)\n",
        "    weights[np.where(mask==0)] = 0.005 #background\n",
        "    weights[np.where(mask==1)] = 1 #adjacent pixels\n",
        "    weights[np.where(mask==2)] = 1 #active pixels\n",
        "    return weights\n",
        "\n",
        "def normalize_image_batch(images):\n",
        "    \"\"\"\n",
        "    Normalize the pixel values of a batch of images to the range [0, 1].\n",
        "    Args:\n",
        "        images: A numpy array of shape (num_images, height, width, num_channels).\n",
        "    Returns:\n",
        "        A numpy array of shape (num_images, height, width, num_channels) with pixel values in the range [0, 1].\n",
        "    \"\"\"\n",
        "    num_images, height, width, num_channels = images.shape\n",
        "    for i in range(num_images):\n",
        "        for j in range(num_channels):\n",
        "            img = images[i, :, :, j]\n",
        "            if np.min(img) != 0 or np.max(img) != 1:\n",
        "                img = rescale_intensity(img, in_range=(np.nanmin(img), np.nanmax(img)), out_range=(0, 1))\n",
        "                img = np.nan_to_num(img, nan=0.0)\n",
        "                images[i, :, :, j] = img\n",
        "    return images\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence): #subclass the Sequence object #subclass or extend?\n",
        "    def __init__(self, x, y, #inputs to generator\n",
        "                 batch_size=2, normalize=True, #arguments used for all datasets in same model run\n",
        "                 augmentation=False, shuffle=False, weights=False, border=0): #arguments used for training only\n",
        "        self.batch_size = batch_size #size of the batch\n",
        "        self.shuffle=shuffle\n",
        "        self.augmentation=augmentation\n",
        "        self.weights=weights\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.border = border\n",
        "        \n",
        "        if self.augmentation: #initialize random augmenter function\n",
        "            self.augmenter = A.Compose( [A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.RandomRotate90(p=1)] ) #8 equally likely possibilities\n",
        "        print(\"Generator initiated with {} sample\".format(len(self.y)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))   #last batch is smaller than others I think?\n",
        "        #return int(np.floor(len(self.x) / float(self.batch_size)))  #all same size batches, leaves out last smaller batch\n",
        "\n",
        "    def __getitem__(self, idx): #returns an entire batch when called, index is the batch number starting from 0\n",
        "        batch_fn = self.x[ idx*self.batch_size : (idx+1)*self.batch_size ] #list of granules\n",
        "        \n",
        "        #open list of files\n",
        "        batch_data = np.array([ np.load(fn)[...,4] for fn in batch_fn ]) #limit to just slant column density\n",
        "        X = np.expand_dims(batch_data, axis=-1) #batch_data.reshape((self.batch_size, 352, 352, 1))\n",
        "        X = normalize_image_batch(X)\n",
        "        plt.imshow(X[0,:,:,0])\n",
        "        global failed_layer \n",
        "        failed_layer = X[0,:,:,0]\n",
        "        plt.colorbar()\n",
        "\n",
        "        assert not np.any(np.isnan(X))\n",
        "        #print(X.shape)\n",
        "        \n",
        "        #BUILD MASKS\n",
        "        vertices = self.y[ idx*self.batch_size : (idx+1)*self.batch_size ] #ready to return\n",
        "        masks = np.array([build_mask(eval(verts), bordersize=self.border) for verts in vertices])\n",
        "        masks = np.expand_dims(masks, axis=-1)\n",
        "        #print(masks.shape)\n",
        "    \n",
        "        #AUGMENTATION\n",
        "        if self.augmentation: #augment each image/mask combo \n",
        "            augmented_all = [self.augmenter(image=im, mask=ma) for im,ma in zip(X, masks)]\n",
        "            X =      np.array( [augmented[\"image\"] for augmented in augmented_all])\n",
        "            masks =  np.array( [augmented[\"mask\"] for augmented in augmented_all])\n",
        "        \n",
        "        #WEIGHTS or not\n",
        "        if self.weights:\n",
        "            weights = np.array([get_weights(mask) for mask in masks])\n",
        "            return X, masks, weights\n",
        "        else: \n",
        "            return X, masks\n",
        "    \n",
        "    def on_epoch_end(self): #shuffles the dataframe at the end of each epoch\n",
        "        if self.shuffle == True:\n",
        "            #reset all the main things, this should probably be done in fewer lines of code\n",
        "            self.df = sklearn_shuffle(self.df, random_state=42) #Using a random_state still shuffles differently every epoch, I checked\n",
        "            self.x = self.df[self.x_col].tolist() #list of ALL granules\n",
        "            self.y = self.df[self.y_col].astype(int).to_numpy() #list of ALL vertices lists\n",
        "            \n",
        "#old train/test split\n",
        "params_list = ['GEOLOCATION/latitude', 'GEOLOCATION/longitude', 'INPUT_DATA/cloud_fraction_crb',\n",
        "                  'OUTPUT_PRODUCTS/cloud_fraction_intensity_weighted',\n",
        "                  'OUTPUT_PRODUCTS/sulfurdioxide_slant_column_corrected', 'OUTPUT_PRODUCTS/sulfurdioxide_total_vertical_column', \n",
        "                  'OUTPUT_PRODUCTS/sulfurdioxide_total_vertical_column_1km', 'OUTPUT_PRODUCTS/sulfurdioxide_total_vertical_column_7km', \n",
        "                  'OUTPUT_PRODUCTS/sulfurdioxide_total_vertical_column_15km']\n",
        "WEIGHTS = False\n",
        "BORDER=0\n",
        "NORMALIZE=True\n",
        "my_generator = DataGenerator(fn_train, verts_train, augmentation=False, shuffle=True, normalize=NORMALIZE)\n",
        "val_generator = DataGenerator(fn_val, verts_val)\n",
        "test_generator = DataGenerator(fn_test, verts_test)\n",
        "\n",
        "generator_output_signature = (tf.TensorSpec(shape=(None, 352, 352, 1), dtype=tf.float32), tf.TensorSpec(shape=(None, 352, 352, 1), dtype=tf.float32) )\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: my_generator, output_signature = generator_output_signature )\n",
        "val_dataset  = tf.data.Dataset.from_generator(lambda: val_generator, output_signature = generator_output_signature)\n",
        "test_dataset   = tf.data.Dataset.from_generator(lambda: test_generator, output_signature = generator_output_signature)\n",
        "\n",
        "\n",
        "# apply prefetching and caching to the datasets\n",
        "prefetch_size = -1 #AUTOTUNE\n",
        "prefetch_dataset = train_dataset.prefetch(prefetch_size) #PREFETCHING IMPLEMENTED\n",
        "val_prefetch = val_dataset.prefetch(prefetch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8fa90a-2630-4a25-a92a-706ce51f0c1c",
      "metadata": {
        "id": "ac8fa90a-2630-4a25-a92a-706ce51f0c1c",
        "outputId": "c4528acd-2c2d-4c62-8ca9-27a8ad604de5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"U-Net\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 352, 352, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 352, 352, 8)  80          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 352, 352, 8)  584         ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 176, 176, 8)  0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 176, 176, 16  1168        ['max_pooling2d[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 176, 176, 16  2320        ['conv2d_2[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 88, 88, 16)  0           ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 88, 88, 24)   3480        ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 88, 88, 24)   5208        ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 44, 44, 24)  0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 44, 44, 32)   6944        ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 44, 44, 32)   9248        ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 22, 22, 32)  0           ['conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 22, 22, 40)   11560       ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 22, 22, 40)   14440       ['conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 11, 11, 40)  0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 11, 11, 48)   17328       ['max_pooling2d_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 11, 11, 48)   20784       ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " tf.image.resize (TFOpLambda)   (None, 22, 22, 48)   0           ['conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 22, 22, 88)   0           ['tf.image.resize[0][0]',        \n",
            "                                                                  'conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 22, 22, 40)   31720       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 22, 22, 40)   14440       ['conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " tf.image.resize_1 (TFOpLambda)  (None, 44, 44, 40)  0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 44, 44, 72)   0           ['tf.image.resize_1[0][0]',      \n",
            "                                                                  'conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 44, 44, 32)   20768       ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 44, 44, 32)   9248        ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " tf.image.resize_2 (TFOpLambda)  (None, 88, 88, 32)  0           ['conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 88, 88, 56)   0           ['tf.image.resize_2[0][0]',      \n",
            "                                                                  'conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 88, 88, 24)   12120       ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 88, 88, 24)   5208        ['conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " tf.image.resize_3 (TFOpLambda)  (None, 176, 176, 24  0          ['conv2d_17[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 176, 176, 40  0           ['tf.image.resize_3[0][0]',      \n",
            "                                )                                 'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 176, 176, 16  5776        ['concatenate_3[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 176, 176, 16  2320        ['conv2d_18[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " tf.image.resize_4 (TFOpLambda)  (None, 352, 352, 16  0          ['conv2d_19[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 352, 352, 24  0           ['tf.image.resize_4[0][0]',      \n",
            "                                )                                 'conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 352, 352, 8)  1736        ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 352, 352, 8)  584         ['conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 352, 352, 2)  18          ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 197,082\n",
            "Trainable params: 197,082\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#U_NET ARCHITECTURE \n",
        "#I removed dropout layers for now. \n",
        "#Normalization still isn't implemented. Not sure how best to do this.\n",
        "#do need to add a 3d convolution in here maybe as a first convolve block?\n",
        "\n",
        "#this time do it better, create functions to minimize code redundancy\n",
        "def double_conv_block(x, n_filters):\n",
        "    # Conv2D then ReLU activation\n",
        "    x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
        "    # Conv2D then ReLU activation\n",
        "    x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
        "    return x\n",
        "def downsample_block(x, n_filters):\n",
        "    f = double_conv_block(x, n_filters)\n",
        "    p = layers.MaxPool2D(2)(f)\n",
        "    #p = layers.Dropout(0.3)(p)\n",
        "    return f, p\n",
        "def upsample_block(x, conv_features, n_filters):\n",
        "    # upsample\n",
        "    #x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
        "    #Get rid of the conv2dtranspose and replace with just upsampling\n",
        "    #using resizing gets rid of 20k params too\n",
        "    #print(x.shape)\n",
        "    width_height = 2*x.shape[1]\n",
        "    x = tf.image.resize(x, [width_height, width_height] , method='nearest')\n",
        "    # concatenate\n",
        "    x = layers.concatenate([x, conv_features])\n",
        "    # dropout\n",
        "    #x = layers.Dropout(0.3)(x)\n",
        "    # Conv2D twice with ReLU activation\n",
        "    x = double_conv_block(x, n_filters)\n",
        "    return x\n",
        "\n",
        "def build_unet_model(INPUTS=1):\n",
        "    # inputs\n",
        "    inputs = layers.Input(shape=(352,352,INPUTS))\n",
        "    #normalize layer, prior to any convolving\n",
        "    #normalized_inputs = layers.BatchNormalization()(inputs) #I REALLY HOPE THIS WORKS!!!!!!\n",
        "    # encoder: contracting path - downsample\n",
        "    \n",
        "    #extra downsample\n",
        "    f0,p0 = downsample_block(inputs,8)\n",
        "    \n",
        "    # 1 - downsample\n",
        "    f1, p1 = downsample_block(p0, 16)\n",
        "    # 2 - downsample\n",
        "    f2, p2 = downsample_block(p1, 24)\n",
        "    # 3 - downsample\n",
        "    f3, p3 = downsample_block(p2, 32)\n",
        "    # 4 - downsample\n",
        "    f4, p4 = downsample_block(p3, 40)\n",
        "    # 5 - bottleneck\n",
        "    bottleneck = double_conv_block(p4, 48)\n",
        "    # decoder: expanding path - upsample\n",
        "    # 6 - upsample\n",
        "    u6 = upsample_block(bottleneck, f4, 40)\n",
        "    # 7 - upsample\n",
        "    u7 = upsample_block(u6, f3, 32)\n",
        "    # 8 - upsample\n",
        "    u8 = upsample_block(u7, f2, 24)\n",
        "    # 9 - upsample\n",
        "    u9 = upsample_block(u8, f1, 16)\n",
        "    \n",
        "    #10 extra upsample block, optional\n",
        "    u10 = upsample_block(u9, f0, 8)\n",
        "    \n",
        "    # OUTPUTS\n",
        "    if BORDER==0: \n",
        "        num_outputs = 2\n",
        "    else: num_outputs = 3\n",
        "    outputs = layers.Conv2D(filters=num_outputs, kernel_size=1, padding=\"same\", activation = \"softmax\")(u10) #\n",
        "    # unet model with Keras Functional API\n",
        "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
        "    return unet_model\n",
        "model = build_unet_model(INPUTS=1)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ac79a6-393d-48f3-85d6-dc4254cc05bf",
      "metadata": {
        "id": "e0ac79a6-393d-48f3-85d6-dc4254cc05bf"
      },
      "outputs": [],
      "source": [
        "#Metrics from thermal project\n",
        "#compile the model, see tutorial for what everything here is\n",
        "class UpdatedMeanIoU(MeanIoU):\n",
        "    def __init__(self,\n",
        "                   y_true=None,\n",
        "                   y_pred=None,\n",
        "                   num_classes=None,\n",
        "                   name=None,\n",
        "                   dtype=None):\n",
        "        super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.argmax(y_pred, axis=-1)\n",
        "        return super().update_state(y_true, y_pred, sample_weight)\n",
        "def class_from_mask(mask):\n",
        "    if BORDER==0:\n",
        "        if tf.reduce_max(mask)==1:\n",
        "            return 1\n",
        "        else: return 0\n",
        "    else: #else 3 classes with borders\n",
        "        if tf.reduce_max(mask) == 2: #image is active,\n",
        "            return 1\n",
        "        else: return 0\n",
        "    \n",
        "def image_acc(y_true, y_pred):\n",
        "    #get image classifications for both true and predicted masks\n",
        "    pred_masks = tf.argmax(y_pred, axis=3) #convert probabilistic predictions to mask\n",
        "    y_true = y_true.numpy()\n",
        "    y_pred = pred_masks.numpy()\n",
        "    true_class = [class_from_mask(mask) for mask in y_true]\n",
        "    pred_class = [class_from_mask(mask) for mask in y_pred]\n",
        "    #now evaluate accuracy \n",
        "    acc = accuracy_score(true_class, pred_class)\n",
        "    return acc\n",
        "\n",
        "def FP(y_true, y_pred):\n",
        "    pred_masks = tf.argmax(y_pred, axis=3) #convert probabilistic predictions to mask\n",
        "    y_true = y_true.numpy()\n",
        "    y_pred = pred_masks.numpy()\n",
        "    true_class = [class_from_mask(mask) for mask in y_true]\n",
        "    pred_class = [class_from_mask(mask) for mask in y_pred]\n",
        "    return 0\n",
        "    \n",
        "def FN(y_true, y_pred):\n",
        "    pass\n",
        "    \n",
        "def image_f1(y_true, y_pred):\n",
        "    #get image classifications for both true and predicted masks\n",
        "    pred_masks = tf.argmax(y_pred, axis=3)\n",
        "    y_true = y_true.numpy()\n",
        "    y_pred = pred_masks.numpy()\n",
        "    true_class = [class_from_mask(mask) for mask in y_true]\n",
        "    pred_class = [class_from_mask(mask) for mask in y_pred]\n",
        "    if 1 in pred_class: #if no active predictions are made the f1 score is undefined\n",
        "        f1 = f1_score(true_class, pred_class)\n",
        "        return f1\n",
        "    else: return 0\n",
        "\n",
        "metrics = [SparseCategoricalAccuracy(), UpdatedMeanIoU(num_classes=3, name='IoU'), image_acc, image_f1]\n",
        "metrics2 = [SparseCategoricalAccuracy(), image_acc]\n",
        "\n",
        "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(learning_rate=0.001), metrics=metrics2, run_eagerly=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84388991-ce6a-4f68-afab-b75a7f860e3a",
      "metadata": {
        "id": "84388991-ce6a-4f68-afab-b75a7f860e3a",
        "outputId": "21f18da2-60d4-4f22-a98f-a79fb36e38cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "    159/Unknown - 129s 804ms/step - loss: 0.0493 - sparse_categorical_accuracy: 0.9942 - image_acc: 0.4780"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(prefetch_dataset, validation_data = val_prefetch, epochs=10, verbose=1) #prefetching, makes it ~10 times faster!\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Total training time is {training_time//60}m : {training_time%60}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaee9414-87a2-4431-8432-ae546fdbed03",
      "metadata": {
        "id": "aaee9414-87a2-4431-8432-ae546fdbed03"
      },
      "outputs": [],
      "source": [
        "#learning rate scheduler\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 200:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * math.exp(-0.05)\n",
        "#def metrics_callback(epoch, lr):\n",
        "    \n",
        "callback = LearningRateScheduler(scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f969be5-05c3-4e37-8014-e1c42d74fed5",
      "metadata": {
        "id": "7f969be5-05c3-4e37-8014-e1c42d74fed5",
        "outputId": "586e5068-7ea5-4ab8-96f4-65ab65771e18"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/k1/039pfytn6x7dgr4j_lhkvs9h0000gn/T/ipykernel_88358/2924102211.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfbuygr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "print(np.max(failed_layer))\n",
        "plt.imshow(fbuygr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8081fac5-4d43-4a62-b7db-9b121e30412e",
      "metadata": {
        "id": "8081fac5-4d43-4a62-b7db-9b121e30412e",
        "outputId": "3bc8fba0-8d9c-42dd-fa48-b5fcb95f33a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 68ms/step\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index 2 is out of bounds for axis 3 with size 2",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/k1/039pfytn6x7dgr4j_lhkvs9h0000gn/T/ipykernel_2431/1026396273.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mactive_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 3 with size 2"
          ]
        }
      ],
      "source": [
        "#test prediction\n",
        "fn =\"training/Western Aleutians/Western_Aleutians__SECTOR_view__20220605T235110.nc\"\n",
        "\n",
        "test_params = params\n",
        "img = open_h5file(fn, params=params)\n",
        "#img = tf.image.resize_with_crop_or_pad(image=img, target_height=256, target_width=256)\n",
        "img = tf.image.resize(images=img, size=[256,256], method='gaussian')\n",
        "\n",
        "prediction = model.predict(np.expand_dims(img, axis=0))\n",
        "active_prob = prediction[0,:,:,2]\n",
        "pred = tf.argmax(prediction[0,:,:,:], axis=2)\n",
        "\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(24,10))\n",
        "axs[0].imshow(img[:,:,0], interpolation=None)\n",
        "prob = axs[1].imshow(active_prob, interpolation=None)\n",
        "plt.colorbar(prob, ax=axs[1])\n",
        "#print(active_prob)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2056ebff-815e-463a-9535-db981441f255",
      "metadata": {
        "id": "2056ebff-815e-463a-9535-db981441f255",
        "outputId": "28bb3937-7425-4820-a846-e5b7275cfba4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fcfa21568b0>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEWCAYAAABFSLFOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxU0lEQVR4nO3de3iddZnv//enSZO2yeo5XYG0kBSyIqBYILAFFGE6OmVUDiP8LKNuqrM3Vg5uxvEnoL9Rt3uzL9nOnkH34E8ZR0EFKgfBjlZQ3KaO1EIPFLDQ9ETapockPadNmzTJvf9YT9LFYuWw0rXyrKx1v64rV9dzvp91pb37fb738/3KzHDOOecyZVzYATjnnMsvnlicc85llCcW55xzGeWJxTnnXEZ5YnHOOZdRnlicc85llCcW54Ygab2kK8OOw7mxQv4ei3POuUzyFotzzrmM8sTi3BAkNUn6c0lfk/SEpJ9Iapf0mqSYpHsktUraIemDCcd9StIbwb5bJX0m6bxflLRb0i5J/0mSSTo72FYq6R8kbZfUIum7kiYOEec0Sb+Q1CbpQPB5dsL26ZJ+GFzvgKRnErZdK2mdpMOStkhakLEv0BUcTyzOpecjwI+BacDLwHPE/x5VAV8HvpewbyvwYWAy8CngnyRdCBD8w/154M+Bs4H3J13nPiAGzAu2VwFfGSK2ccAPgTOBM4BjwD8nbP8xMAk4D5gF/FMQyyXAj4D/F5gKXAE0DXEt5wbkfSzODUFSE/CfgPcCl5vZB4L1HwEeA6aYWY+kCHAYmGZmB1Oc5xngd2b2LUk/AFrM7J5g29nAJqAW2AIcAc43sy3B9kuBR82sJo245wXXmybpNGAnMMPMDiTt9z2gw8z+drjndm4wxWEH4NwY05Lw+Riw18x6EpYByoGDkq4Gvkq85TGOeGvhtWCf04HVCefakfC5Ith3jaS+dQKKBgtM0iTirZAFxFtUABFJRcAcYH9yUgnMAZYNdm7n0uGPwpzLAkmlwFPAPwBRM5tK/B/vvkyxG5idcMichM97iSep88xsavAzxczKh7js3wF1wH8ws8nEH2kRXHMHMF3S1BTH7QDOGu69OTcUTyzOZUcJUAq0Ad1B6+WDCdsfBz4l6ZygpdHff2JmvcC/EO+TmQUgqUrSXwxxzQjxhHRQ0nTiraW+c+4GfgV8J+jkHy+pL/H8axDLfEnjgmu94xTu3RU4TyzOZYGZtQOfI55ADgB/DSxN2P4r4NvA74DNwB+DTZ3Bn3cF61dKOgw8T7w1Mpj7gYnEWzwrgWeTtn8SOAFsIF5YcGcQy0sExQXAIWA58QIA50bEO++dywGSzgH+BJSaWXfY8Th3KrzF4lxIJF0vqUTSNOLlxf/mScXlA08szoXnM8T7YLYAPcBnhzpA0pckHUnx86tsB+vccPmjMOeccxnlLRbnnHMZVfAvSM6cOdOqq6vDDsM558aUNWvW7DWzilTbCj6xVFdXs3r16qF3dM4510/StoG2+aMw55xzGeWJxTnnXEZ5YnHOOZdRBd/H4pxz6Tpx4gTNzc0cP3487FCybsKECcyePZvx48cP+xhPLM45l6bm5mYikQjV1dUkTG2Qd8yMffv20dzcTE3NsKcC8kdhzjmXruPHjzNjxoy8TioAkpgxY0baLTNPLM45NwL5nlT6jOQ+/VGYcy6r9h3p5JEXt9Pd0xt2KJxz2mSuftdpYYeR9zyxOOey6scrt3H/85sI+z/4ZlBaPI4PnldJ0bix3drYt28f8+fPB2DPnj0UFRVRURF/Cf6ll16ipKRkwGNXr17Nj370I7797W9nLT5PLM65rPpdYxvz5kzlmdsuDzWOJ9c084UnXmHbvqPMrRhqlufcNmPGDNatWwfA1772NcrLy/nCF77Qv727u5vi4tT/vNfX11NfX5/V+LyPxTmXNfuOdPJq80GurEs5pNSoikXjyWRjS3vIkWTHokWL+PznP89VV13FXXfdxUsvvcRll13GBRdcwGWXXUZjYyMADQ0NfPjDHwbiSenTn/40V155JXPnzs1YK8ZbLM65rPn3TXsxgyvrZoUdCmfPKkeCxj1HWPDOzJ33v/7bel7fdThzJwTOPX0yX/3IeWkft3HjRp5//nmKioo4fPgwv//97ykuLub555/nS1/6Ek899dTbjtmwYQO/+93vaG9vp66ujs9+9rNpvbOSiicW51zWNDS2Mr2shPOrpoQdCpNKijlj+qS8bbEA3HjjjRQVFQFw6NAhbr75ZjZt2oQkTpw4kfKYD33oQ5SWllJaWsqsWbNoaWlh9uzZpxSHJxbnXFb09hq/37SXK2pnMi5HOstj0UjGE8tIWhbZUlZW1v/57//+77nqqqt4+umnaWpq4sorr0x5TGlpaf/noqIiurtPfXZs72NxzmXFqzsPsf9oV048ButTF43w5t6jdHb3hB1K1h06dIiqqioAHnrooVG9ticW51xWNDS2IsEVsfA77vvURsvp7jXe3Hs07FCy7otf/CL33HMPl19+OT09o5tIsz7nvaQFwLeAIuD7ZvaNAfa7GFgJfMzMnpRUB/w0YZe5wFfM7P6EY74AfBOoMLO9wbp7gL8BeoDPmdlzg8VXX19vPtGXc5l33QMvYMDPQy4zTrRhz2EW3P/vfGvhPK6dVzXi87zxxhucc845GYwst6W6X0lrzCxl3XJW+1gkFQEPAB8AmoFVkpaa2esp9rsP6E8CZtYIzEvYvhN4OuGYOcF5tyesOxdYCJwHnA48LylmZvnf7nUuh+w/2sUrzQf53J/Vhh3KW8ydWU7xOOV1B34uyPajsEuAzWa21cy6gCXAtSn2uwN4Cmgd4DzzgS1mljgV5j8BXwQSm1zXAkvMrNPM3gQ2BzE450bRv29qC8qMc+cxGEBJ8ThqZpaxseVI2KHktWwnlipgR8Jyc7Cun6Qq4Hrgu4OcZyHwWMIx1wA7zeyVdK8XHH+LpNWSVre1tQ3nPpxzaWhobGPapPGcP3tq2KG8TawyM5Vh2e5GyBUjuc9sJ5ZUNYbJUd4P3DXQ4ypJJcA1wBPB8iTgy8BXRng9zOxBM6s3s/q+8XWcc5nR22v8fmMbV8QqcnJMrtisCNv3d9DRNfKy2gkTJrBv3768Ty5987FMmDAhreOy/R5LMzAnYXk2sCtpn3pgSTA080zgLyV1m9kzwfargbVm1hIsnwXUAK8Ex8wG1kq6ZJjXc85l0Ws7D7HvaFfOPQbrU1dZjhlsbj0y4hbV7NmzaW5uphCeePTNIJmObCeWVUCtpBrine8Lgb9O3MHM+qclk/QQ8IuEpAJwEwmPwczsNWBWwjFNQL2Z7ZW0FHhU0j8S77yvBV7K7C055wbT0NgWLzOuzc3EEotGAGjc0z7ixDJ+/Pi0ZlQsNFlNLGbWLel24tVeRcAPzGy9pMXB9sH6Vfoee30A+Mwwr7de0uPA60A3cJtXhDk3uho2tnJ+1RRmlJcOvXMIzpxRRknxODa1egd+tmR9SBczWwYsS1qXMqGY2aKk5Q5gxhDnr05avhe4dwShOudO0YGjXazbcZA7cqzMOFHROFE7q5zGPV5ynC3+5r1zLmN+n6NlxsmyMWaYO8kTi3MuY5YHZcbvzsEy40SxaITdh45z6FjqEX/dqfHE4pzLiN5eY/nGNt5Xm5tlxonqKuOTfm3yVktWeGJxzmXEn3bldplxor7KMH8DPzs8sTjnMqKhMf5ORy6NZjyQqqkTKSsp8n6WLPHE4pzLiIbGVs6fPYWZOVpmnEgStdGIV4ZliScW59wpO9gRLzO+cgy0VvrUeWVY1nhicc6dst9v2kuvwftzaLbIocQqI+w72sXeI51hh5J3PLE4505ZQ2MrUyeNZ96cqWGHMmx1/R343mrJNE8szrlT0jea8VgoM04UC0qON3o/S8Z5YnHOnZL1uw6z90jXmOpfAagoL2XqpPE0eslxxnlicc6dkobG+MSvY6HMOJEkH9olSzyxOOdOScPGNt5VNYWKSO6XGSeri0bYuKc97yfsGm2eWJxzI3awo4uXtx8YE2/bpxKrjNDe2c2ew8fDDiWveGJxzo3YvwdlxmM1sdQlTPrlMscTi3NuxBoa25gycTzz5kwLO5QRiUWDyjDvZ8morCcWSQskNUraLOnuQfa7WFKPpBuC5TpJ6xJ+Dku6M9j23yS9Gqz/taTTg/XVko4lHDPoDJXOuZE7OZrxzDFVZpxo6qQSZkVKadzjlWGZlNUZJCUVAQ8Qn164GVglaamZvZ5iv/uIT2EMgJk1AvMStu8Eng42f9PM/j7Y9jngK8DiYNsWM5uXpVtyzgVe332YvUc6uXIMvW2fSl2lV4ZlWrZbLJcAm81sq5l1AUuAa1PsdwfwFNA6wHnmE08Y2wDM7HDCtjLASzqcG2XLN8ZHM37/GCszThaLRtjU2k5vr/8zkinZTixVwI6E5eZgXT9JVcD1wGCPrRYCjyUdd6+kHcDHibdY+tRIelnScknvO5XgnXMDa2hs5Z1Vk8dkmXGiWLSc4yd62XGgI+xQ8ka2E0uqB6/J/y24H7jLzHpSnkAqAa4BnnjLScy+bGZzgEeA24PVu4EzzOwC4PPAo5ImpzjnLZJWS1rd1taWzv0454BDx06wdvtBroyN7cdgcHLSL68My5xsJ5ZmYE7C8mxgV9I+9cASSU3ADcB3JF2XsP1qYK2ZtQxwjUeBjwKYWaeZ7Qs+rwG2ALHkA8zsQTOrN7P6ioqx3Yx3Lgx/2LSXnl4bs2XGiWp9MMqMy3ZiWQXUSqoJWh4LgaWJO5hZjZlVm1k18CRwq5k9k7DLTbz9MVhtwuI1wIZgfUXQ0Y+kuUAtsDWjd+Sco6GxlckTisfUaMYDKS8tZva0iT5mWAZltSrMzLol3U682qsI+IGZrZe0ONg+aDmwpEnEK8o+k7TpG5LqgF5gGycrwq4Avi6pG+gBFpvZ/ozdkHMOs6DMOFZBcVF+vApXF42wyVssGZPVxAJgZsuAZUnrUiYUM1uUtNwBzEix30cHOP4p4tVlzrkseX33YVrbO8fcaMaDqY1G+P2mNk709DI+T5JlmPwbdM6lpaExKDPOg/6VPnWV5ZzoMZr2Hg07lLzgicU5l5bljW2cd/pkZkUmhB1KxvRXhvnjsIzwxOKcG7ZDx06wZgyPZjyQsyrKGSefTTJTPLE454bthc19ZcZj//2VRBPGF1E9s4yNXhmWEZ5YnHPD1ldmfEEelBkni83yMcMyxROLc25Y+suMa/OnzDhRrDJC076jHD+RchAQl4b8++1wzmXFG7vbaTncmVfVYInqohF6DTa3+uOwU+WJxTk3LA0b44OP59P7K4nqKn3Sr0zxxOKcG5aGxjbOPW0ysybnT5lxojNnlFFSNM478DPAE4tzbkiHj59gzbb8KzNONL5oHHMryrzFkgGeWJxzQ3phU36WGSeLRSM+fH4GeGJxzg2pobGNyIRiLjxjatihZFVdZYSdB4/RfvxE2KGMaZ5YnHODOllmPDMvy4wT9Q3tsskrw05Jfv+WOOdO2YY97ew5fDwvZoscSl1fYvF+llPiicU5N6h8HM14ILOnTWTi+CIa93iL5VR4YnHODaqhsZVzTptMNE/LjBONGydqo+VeGXaKPLE45wbUXgBlxsli0YgPn3+Ksp5YJC2Q1Chps6S7B9nvYkk9km4IluskrUv4OSzpzmDbf5P0arD+15JOTzjPPcG1GiX9Rbbvz7l89sLmvXT3Wt6+bZ9KXTRCW3snB452hR3KmJXVxCKpCHgAuBo4F7hJ0rkD7Hcf8FzfOjNrNLN5ZjYPuAjoAJ4ONn/TzM4Ptv0C+EpwnnOBhcB5wALgO8G5nXMj0NDYRqS0mAvPnBZ2KKMmVhnvwPfHYSOX7RbLJcBmM9tqZl3AEuDaFPvdQXyu+tYBzjMf2GJm2wDM7HDCtjLAgs/XAkvMrNPM3gQ2BzE459JkZjQ0tvHe2pkFNQ98LOpjhp2qbP+2VAE7Epabg3X9JFUB1wPfHeQ8C4HHko67V9IO4OMELZbhXC849hZJqyWtbmtrG+atOFdYGluCMuMC6l8BqJw8gciEYu9nOQXZTixKsc6Slu8H7jKzlJMgSCoBrgGeeMtJzL5sZnOAR4Db07geZvagmdWbWX1FRWH9pXFuuPrLjAvg/ZVEkqiLRtjoJccjlu3E0gzMSVieDexK2qceWCKpCbiBeL/IdQnbrwbWmlnLANd4FPhoGtdzzg1DQ2Mr76iMUDkl/8uMk8Uq45VhZm/7f6kbhmwnllVAraSaoOWxEFiauIOZ1ZhZtZlVA08Ct5rZMwm73MTbH4PVJixeA2wIPi8FFkoqlVQD1AIvZfB+nCsI7cdPsLrpQN4POjmQumiEQ8dO0NbeGXYoY1JxNk9uZt2Sbide7VUE/MDM1ktaHGwfrF8FSZOADwCfSdr0DUl1QC+wDeg733pJjwOvA93AbQM9YnPODeyFzfviZcYF1r/SpzbowG9sac/b+WeyKauJBcDMlgHLktalTChmtihpuQOYkWK/jyavS9h2L3DvSGJ1zsUt39hKpLSYiwqozDhR35hhjXvaeV9tYSbXU1E4NYTOuWHpKzO+/OzCKjNONKO8lJnlJV5yPEKF+VvjnBvQxpYj7D5UeGXGyeJDu3hl2Eh4YnHOvUVDY/w95UIYzXgwsWiEzS3t9PZ6ZVi6PLE4596iobGNd1RGOG3KxLBDCVUsGuFoVw87Dx4LO5QxxxOLc67fkc5uVm/bX/CtFYC6Sh/aZaQ8sTjn+r2weS8neqwgZoscSm1fZZgnlrR5YnHO9WtobKO8tJj66sIsM040ecJ4Tp8ygY17PLGkyxOLcw6Ilxkvb2zl8rNnFGyZcbJYZYSNXhmWtmH/9ki6TdLUhOVpkm7NSlTOuVG3qfUIuw4dL9hhXFKJRSNsbjtCd09v2KGMKen8t+Q/m9nBvgUzOwD854xH5JwLRV+ZcaG/v5IoFo3Q1d3Ltv0dYYcypqSTWMZJ6h+WPpiZsSTzITnnwtDQ2EZd1MuME/UN7eL9LOlJJ7E8Bzwuab6kPyM+4vCz2QnLOTeajnR2s6ppv7dWkpw9qxzJK8PSlc4glHcBtwCfJT6h1q+B72cjKOfc6FoRlBn7+ytvNbGkiDOnT2KTd+CnJZ3EMhH4l76RiYNHYaWAP3x0boxr2NhGWUkR9WdODzuUnFMbjXiLJU3pPAr7LfHk0mci8Hxmw3HOjbZ4mXF8NOOSYi8zTlYXjfDm3qN0dvvUTsOVzm/RBDPrbw8GnydlPiTn3Gja3HqEnQePeZnxAGKVEXp6ja1tR8MOZcxIJ7EclXRh34Kki4AhR2eTtEBSo6TNku4eZL+LJfVIuiFYrpO0LuHnsKQ7g23flLRB0quSnu57v0ZStaRjCccMOkOlcy5eDQZeZjyQ/sowfxw2bOn0sdwJPCFpV7B8GvCxwQ4I+mEeID69cDOwStJSM3s9xX73Ea88A8DMGoF5Cdt3Ak8Hm38D3BNMfXwfcA/x4gKALWY2L437cq6gNWxsJRYt5/SpXmacSs3MMorHyRNLGoadWMxslaR3AHXEq8I2mNmJIQ67BNhsZlsBJC0BriU+J32iO4CngIsHOM984gljWxDLrxO2rQRuGO59OOdOOtrZzao3D7Do8uqwQ8lZJcXjqJlZRuMerwwbrnR76uqAc4ELgJsk/cch9q8CdiQsNwfr+kmqAq4HBntstZD4ezOpfBr4VcJyjaSXJS2X9L5UB0i6RdJqSavb2tqGuAXn8teKLfvo6unlypg/BhtMfMwwb7EMVzpjhX0V+N/Bz1XA/wSuGeqwFOuSp2O7H7jLzFKWXEgqCa7zRIptXwa6gUeCVbuBM8zsAuDzwKOSJr8tALMHzazezOorKvwvlCtcDY2t8TLjai8zHkxdNML2/R10dHWHHcqYkE6L5Qbij6T2mNmngHcTf49lMM3AnITl2cCupH3qgSWSmoJrfEfSdQnbrwbWmllL4kGSbgY+DHzczAzAzDrNbF/weQ2wBYgN9wadKyRmRkNjG5d5mfGQYkEHvr8oOTzp/DYdM7NeoDtoBbQCc4c4ZhVQK6kmaHksBJYm7mBmNWZWbWbVwJPArWb2TMIuN5H0GEzSAuKd9deYWUfC+oqgox9Jc4FaYGsa9+hcwdjS1ldm7K32ocSiPptkOtKpClsdlPX+C7AGOAK8NNgBQdXW7cSrvYqAH5jZekmLg+2DlgNLmkS8ouwzSZv+mXhr6TfBuJgrzWwxcAXwdUndQA+w2Mz2p3GPzhWMk2XG/v7KUM6cUUZJ8ThPLMOUTlVY39wr35X0LDDZzF7t2y7pPDNbn+K4ZcCypHUpE4qZLUpa7gBmpNjv7AGOf4p4dZlzbggNjW3UziqnysuMh1Q0TtTOKqfRH4UNy4gerJpZU2JSCfw4A/E450bB0c5uXnrTRzNOR1004sPnD1Mme+xSVYA553LQH/vKjP0x2LDFKiPsOXycQ8eGen3PZTKxJJcRO+dyVMPGViaVFFFfPS3sUMaMvg78Td7PMiSvMXSuwPSXGZ81k9LiorDDGTP6So59CP2hZTKxdGXwXM65LNnSdpTmA15mnK6qqRMpKynyfpZhSOfNe0n6hKSvBMtnSLqkb7uZvScbATrnMquhsRXw0YzTJYlYpU/6NRzptFi+A1xK/IVFgHbiIxc758aQ5RvbOHtWObOn+XRK6aqLRtjoJcdDSiex/Aczuw04DmBmB4CSrETlnMuKjq5uXty63wedHKHaaIT9R7vYe6Qz7FByWjqJ5UQwXIpBfPgUoDcrUTnnssLLjE9N/6Rf3s8yqHQSy7eJT7Q1S9K9wB+A/5GVqJxzWdHQ2MakkiIurvEy45GIVcZLjr2fZXDpDOnyiKQ1xEc4FnCdmb2RtciccxllZjRsbOWys2Z4mfEIVZSXMm3SeB8zbAjDTiySphMf0fixhHXjhzGLpHMuB2zde5Qd+49xyxVnhR3KmCWJWDRCoz8KG1Q6j8LWAm3ARmBT8PlNSWslXZSN4JxzmdM/mrF33J+SWDTCppYjBNNAuRTSSSzPAn9pZjPNbAbxCbgeB24lXorsnMthDY2tnFVRxpzpXmZ8KmKVEdo7u9l96HjYoeSsdBJLvZk917dgZr8GrjCzlQw9k6RzLkTHunp48c39Xg2WAXU+tMuQ0kks+yXdJenM4OeLwIGgBNnLjp3LYX/cupeu7l5/2z4D+meT9H6WAaWTWP6a+Jz1zwA/B84I1hUB/89AB0laIKlR0mZJdw+y38WSeiTdECzXSVqX8HNY0p3Btm9K2iDpVUlPBzNb9p3nnuBajZL+Io37cy5vNTS2MXF8EZfUTA87lDFv6qQSopNLvcUyiHTKjfcCdwyweXOqlUFr5gHi0ws3A6skLTWz11Psdx/xKYz7rtcIzEvYvpP4ezQAvwHuCaY+vg+4B7hL0rnAQuA84HTgeUkxM+sZ7n06l29OjmbsZcaZ0teB71JLZxDKiqClsEzS/+n7GeKwS4DNZrbVzLqAJcC1Kfa7g/iUwq0DnGc+sMXMtkG8f8fMuoNtK4m3pAjOvcTMOs3sTeIJ75K3nc25AvLm3qNs39/hj8EyKBaNsKm1nZ5erwxLJZ1HYY8AG4Aa4L8CTcCqIY6pAnYkLDcH6/pJqgKuB747yHkWkvD+TJJPA78a7vWcKzT9ZcbecZ8xddEIx0/0smN/R9ih5KR0EssMM/tX4ISZLTezTwNDDZWfarri5BR/P3DXQI+rJJUA1wBPpNj2ZaCbeNIb7vWQdIuk1ZJWt7W1DRy9c3mgYWMbc73MOKNilV4ZNpi0BqEM/twt6UOSLuDkI6iBNANzEpZnA7uS9qkHlkhqAm4AviPpuoTtVwNrzawl8SBJNwMfBj5uJ99UGs71MLMHzazezOorKvzxgMtfx7p6WLl1H1fGvLWSSbWzvDJsMMPuvAf+u6QpwN8B/xuYDPztEMesAmol1RDvfF9IvJKsn5nV9H2W9BDwCzN7JmGXm0h6DCZpAXAX8H4zS2yLLgUelfSPxDvva4GXhnl/zuWdlVv3eZlxFpSVFjN72kQ2tnoHfirpVIX9Ivh4CLhqmMd0S7qdeLVXEfADM1svaXGwfbB+FSRNIl5R9pmkTf9M/KXM30gCWGlmi4NzPw68TvwR2W1eEeYKWUNjq5cZZ0ldNOItlgGkMwhlDfHqrerE48zsmsGOM7NlwLKkdSkTipktSlruAGak2O/sQa53L3DvYDE5VygaNrZx6VkzmDDey4wzLVYZYfnGNrq6eykpTqdXIf+l8yjsGeBfgX/D37R3Ludtbj3Ctn0d/M17a4be2aWtLhqhu9do2neUWDDMi4tLJ7EcN7NvZy0S51xGPfridorHiQXnVYYdSl7qSyaNe9o9sSRJJ7F8S9JXgV8D/RM+m9najEflnDslRzu7eWL1Dv7yXacxa/KEsMPJS3Mryhgn2OQlx2+TTmJ5F/BJ4M84+SjMgmXnXA752dpm2ju7WXR5ddih5K0J44uonlnm77KkkE5iuR6YGwzN4pzLUWbGQyuaOH/2FC6YMzXscPJaXTTCBq8Me5t0ShleAaZmKQ7nXIb8YfNetrQdZdFl1QTl+C5LYtEITfuOcvyEv9WQKJ0WSxTYIGkVb+1jGbTc2Dk3uh5e0cTM8hI+dP5pYYeS9+oqI5jFK/DeWTUl7HByRjqJ5atZi8I5lxHb93Xw2w2t3HHV2T5E/ijon/Srpd0TS4J03rxfns1AnHOn7kd/bKJI4uPvOTPsUArCmTPKKCka5x34SYZMLJL+YGbvldTOW0cKFmBmNjlr0Tnnhu1oZzc/Xb2Dq991GlEvMR4V44vGMbeizId2STJkYjGz9wZ/+htAzuWwp1/eSfvxbhZd5q2V0VRXGWF104Gww8gpPsCNc3nAzHh4RRPvqprChWdMCzucghKLRth58Bjtx08MvXOB8MTiXB5YsWUfm1qPcLOXGI+6vuFcNvkQ+v08sTiXB374QhMzykr4sJcYj7q6ILF4P8tJnlicG+N27O/gtxtauOmSM3x4/BDMnjaRieOLvDIsgScW58a4H/2xiXESn/AS41CMGydi0XI2emLp54nFuTGso6ubn67awYJ3VlI5xUuMw1IbjdC4x/tY+mQ9sUhaIKlR0mZJdw+y38WSeiTdECzXSVqX8HNY0p3BthslrZfUK6k+4RzVko4lHDPo1MfOjXVPv7yTw8e7+dRl1WGHUtDqohH2Hulk/1EfoxfSG9IlbZKKgAeIz1vfDKyStNTMXk+x333Ac33rzKwRmJewfSfwdLD5T8BfAd9LcdktZjYvozfiXA7qKzE+7/TJXHSmlxiHKVYZdOC3tPOeuW+bTb3gZLvFcgmw2cy2BsPtLwGuTbHfHcBTQOsA55lPPGFsAzCzN4LE41zB+uOWfWxsOeKjGOeA/sow72cBsp9YqoAdCcvNwbp+kqqIz/Uy2GOrhcBjw7xmjaSXJS2X9L5UO0i6RdJqSavb2tqGeVrncstDK5qYXlbCR959etihFLzo5FImTyim0UuOgewnllT/jbKk5fuBu8ws5YQGkkqAa4AnhnG93cAZZnYB8HngUUlvG8vMzB40s3ozq6+oqBjGaZ3LLTv2d/D8Gy3cdMkcLzHOAZKIRSPeYglktY+FeAtlTsLybGBX0j71wJKgKT8T+EtJ3Wb2TLD9amCtmbUMdTEz6ySYK8bM1kjaAsSA1adyE87lmp+s3Ia8xDinxCoj/PLV3ZhZwT+azHaLZRVQK6kmaHksBJYm7mBmNWZWbWbVwJPArQlJBeAmhvkYTFJF0NGPpLlALbD1lO/CuRxyrKuHJat2sOC8Sk6bMjHscFygLhrh0LETtLZ3Dr1znstqYjGzbuB24tVebwCPm9l6SYslLR7qeEmTiFeU/Sxp/fWSmoFLgV9K6qsmuwJ4VdIrxJPUYjPbn7k7ci58z6zbyaFjJ7jZS4xzSt+YYd7Pkv1HYZjZMmBZ0rqUHfVmtihpuQN4W+2emT3NydLjxPVPEa8ucy4vmRkPvdDEuadN5uJqLzHOJYmzSV4RK+y+W3/z3rkxZOXW/TS2tHuJcQ6aUV7KzPISb7HgicW5MeWhFW8ybdJ4rpnnJca5KBaNsNGHz/fE4txY0Xygg9+83sJCH8U4Z8WiETa1tNPbm/xWRWHxxOLcGPFjLzHOeXWVETq6eth58FjYoYTKE4tzY8Cxrh5+umoHHzw3StVULzHOVV4ZFueJxbkx4OfrdnKw4wSLvMQ4p9UGlWGFPumXJxbncpyZ8dCKJt5RGeGSmulhh+MGMXnCeE6fMqHgh3bxxOJcjnvxzf1s2NPOpy73EuOxIFYZYWNLYVeGeWJxLsc9vKKJqZPGc+28qqF3dqGri0bY0nqE7p7esEMJjScW53LYzoPHeG79HhZe7CXGY0UsGqGrp5emfR1hhxIaTyzO5bCfrNwGwCfec0bIkbjhivmkX55YnMtVx0/08NhL2/nguZXMnjYp7HDcMJ09qxypsEuOPbE4l6OWrtvFwQ4fxXismVhSxJnTJ7Gp1ROLcy6HmBk/DEqM3zPXS4zHmlg04i0W51xuWdV0gDd2H+ZmH8V4TKqrjNC0r4PjJ1LOuJ73PLE4l4MeWvEmUyaO5zovMR6TaqMRenqNrW1Hww4lFFlPLJIWSGqUtFnS3YPsd7GkHkk3BMt1ktYl/ByWdGew7UZJ6yX1SqpPOs89wbUaJf1FVm/OuSzYdfAYz61vYeHFc5hY4iXGY1FdgVeGZXUGyWD++QeITy/cDKyStNTMXk+x333EpzAGwMwagXkJ23dyctbIPwF/BXwv6TznAguB84DTgeclxcysMNujbkz6ycptmJmPYjyG1cwso3icCjaxZLvFcgmw2cy2mlkXsAS4NsV+dxCfUrh1gPPMB7aY2TYAM3sjSDzJrgWWmFmnmb0JbA5icG5MOH6ihyWrdvDn50SZM91LjMeqkuJxzK0o88SSJVXAjoTl5mBdP0lVwPXAdwc5z0LgsUxcL7jmLZJWS1rd1tY2jNM6Nzr+7ZVd7D/a5aMY54FYNFKwoxxnO7GkKmdJnlrtfuCugR5XSSoBrgGeyND1MLMHzazezOorKiqGcVrnsq9vFONYtJxLz5oRdjjuFMWiEXbsP8bRzu6wQxl12U4szcCchOXZwK6kfeqBJZKagBuA70i6LmH71cBaM2vJ0PWcy0lrth1g/S4vMc4XfUO7bGotvJGOs51YVgG1kmqClsdCYGniDmZWY2bVZlYNPAncambPJOxyE8N7DEZw7oWSSiXVALXAS6d4D86Nih+uaGLyhGKuv8BLjPNBXWXhVoZlNbGYWTdwO/FqrzeAx81svaTFkhYPdbykScQryn6WtP56Sc3ApcAvJT0XXG898DjwOvAscJtXhLmxYPehYzz7pz187OI5TCrJarGmGyVnTJ9EafE4NhbgG/hZ/w02s2XAsqR1KTvqzWxR0nIH8LaHzWb2NCdLj5O33QvcO8JwnQvFIyu302vGf7y0OuxQXIYUjRO10fKC7MD3N++dC1nfKMbz3+ElxvkmNivij8Kcc6PvF6/uZt/RLj51eXXYobgMi1VGaDncyaGOE2GHMqo8sTgXIjPj4RVN1M4q5zIvMc47/UO7FNgQ+p5YnAvR2u0HeG3nIS8xzlOxoDKs0IbQ98TiXIgeWrGNiJcY563Tp0ygvLS44PpZPLE4F5KWw8f51Wu7+Vj9HMpKvcQ4H0lBZZi3WJxzo+GRldvo8RLjvFcXjVeGmb1tdKm85YnFuRB0dvfw6Evbmf+OWZwxw0uM81ksGuFAxwn2HukKO5RR44nFuRD88tXd7D3Sxc0+inHeK8ShXTyxODfK+kYxPntWOe89e2bY4bgsq42WA4VVGeaJxblR9vKOg7zafIibLz3TS4wLQEV5KdMmjfcWi3Muex56oYlIaTF/deHssENxo0BSwU365YnFuVHUcvg4y17bzY1eYlxQ6iojbGo5UjCVYZ5YnBtFj7y4PSgxPjPsUNwoikUjHOnsZteh42GHMio8sTg3Sjq7e3j0xe1cVTeL6pllYYfjRlHfbJKFMjeLJxbnRsmy13az90gni7zEuODE+irDCqSfJeuJRdICSY2SNku6e5D9LpbUI+mGYLlO0rqEn8OS7gy2TZf0G0mbgj+nBeurJR1LOCblhGLOheGhFduYW1HmJcYFaOqkEqKTS73FkgmSioAHgKuBc4GbJJ07wH73EZ/CGAAzazSzeWY2D7gI6ODkrJF3A781s1rgt8Fyny19x5nZkNMfOzcaXt5+gFd2HGTRZdWMG+clxoUoFo0UzPD52W6xXAJsNrOtZtYFLAGuTbHfHcBTQOsA55lPPGFsC5avBR4OPj8MXJexiJ3LgodXNFHuJcYFrS4arwzr6c3/yrBsJ5YqYEfCcnOwrp+kKuB6YLDHVguBxxKWo2a2GyD4c1bCthpJL0taLul9qU4m6RZJqyWtbmtrG/7dODcCre3H+eVru7mxfjblXmJcsGLRCJ3dvWzf3xF2KFmX7cSSqs2fnK7vB+4ys56UJ5BKgGuAJ4Zxvd3AGWZ2AfB54FFJk98WgNmDZlZvZvUVFRXDOK1zI/foi9s50eOjGBe6Qpr0K9uJpRmYk7A8G9iVtE89sERSE3AD8B1J1yVsvxpYa2YtCetaJJ0GEPzZCmBmnWa2L/i8BtgCxDJ2N86lqau7l0de3M5VdRXUeIlxQaudFa8MK4ShXbKdWFYBtZJqgpbHQmBp4g5mVmNm1WZWDTwJ3GpmzyTschNvfQxGcI6bg883Az8HkFQRFAIgaS5QC2zN6B05l4Zf/Wk3be2dPoqxo6y0mDnTJxZEYsnqA18z65Z0O/FqryLgB2a2XtLiYPug5cCSJgEfAD6TtOkbwOOS/gbYDtwYrL8C+LqkbqAHWGxm+zN2Q86l6YcvNDF3ZhlX1PojV3dy0q98l/WeRDNbBixLWpcyoZjZoqTlDmBGiv32Ea8US17/FPHqMudCt27HQdbtOMjXPnKulxg7AGqjERoa2+jq7qWkOH/fT8/fO3MuZH0lxh+9yEuMXVxdNEJ3r/Hm3qNhh5JVnlicy4LW9uP84tVd3HDRbCITxocdjssRfWOG5fvQLp5YnMuCx17cEZQY+yjG7qS5FWUUjROb8jyx+NtaI7Riy16++vP1YYfhclTzgWO8P1bB3IrysENxOWTC+CKqZ0zioRVNPPunPWGHw/UXVnHrlWdn/LyeWEaovLS4fy5r55LVVUZY/P6zwg7D5aDPza/lufXhJxWAmeWlWTmvCmVGs4HU19fb6tWrww7DOefGFElrzKw+1TbvY3HOOZdRnlicc85llCcW55xzGeWJxTnnXEZ5YnHOOZdRnlicc85llCcW55xzGeWJxTnnXEYV/AuSktqAbSM8fCawN4PhjHX+fbyVfx8n+XfxVvnwfZxpZiknGir4xHIqJK0e6M3TQuTfx1v593GSfxdvle/fhz8Kc845l1GeWJxzzmWUJ5ZT82DYAeQY/z7eyr+Pk/y7eKu8/j68j8U551xGeYvFOedcRnlicc45l1GeWEZI0gJJjZI2S7o77HjCJGmOpN9JekPSekn/JeyYwiapSNLLkn4RdixhkzRV0pOSNgS/I5eGHVNYJP1t8HfkT5IekzQh7JiywRPLCEgqAh4ArgbOBW6SdG64UYWqG/g7MzsHeA9wW4F/HwD/BXgj7CByxLeAZ83sHcC7KdDvRVIV8Dmg3szeCRQBC8ONKjs8sYzMJcBmM9tqZl3AEuDakGMKjZntNrO1wed24v9wVIUbVXgkzQY+BHw/7FjCJmkycAXwrwBm1mVmB0MNKlzFwERJxcAkYFfI8WSFJ5aRqQJ2JCw3U8D/kCaSVA1cALwYcihhuh/4ItAbchy5YC7QBvwweDT4fUllYQcVBjPbCfwDsB3YDRwys1+HG1V2eGIZGaVYV/B125LKgaeAO83scNjxhEHSh4FWM1sTdiw5ohi4EPj/zewC4ChQkH2SkqYRf7JRA5wOlEn6RLhRZYcnlpFpBuYkLM8mT5u0wyVpPPGk8oiZ/SzseEJ0OXCNpCbij0j/TNJPwg0pVM1As5n1tWCfJJ5oCtGfA2+aWZuZnQB+BlwWckxZ4YllZFYBtZJqJJUQ74BbGnJMoZEk4s/Q3zCzfww7njCZ2T1mNtvMqon/XvwfM8vL/5UOh5ntAXZIqgtWzQdeDzGkMG0H3iNpUvB3Zj55WshQHHYAY5GZdUu6HXiOeGXHD8xsfchhhely4JPAa5LWBeu+ZGbLwgvJ5ZA7gEeC/4RtBT4VcjyhMLMXJT0JrCVeSfkyeTq0iw/p4pxzLqP8UZhzzrmM8sTinHMuozyxOOecyyhPLM455zLKE4tzzrmM8sTi3Bgj6UofNdnlMk8szjnnMsoTi3NZIukTkl6StE7S94I5Wo5I+l+S1kr6raSKYN95klZKelXS08G4Ukg6W9Lzkl4JjjkrOH15whwnjwRvciPpG5JeD87zDyHduitwnlicywJJ5wAfAy43s3lAD/BxoAxYa2YXAsuBrwaH/Ai4y8zOB15LWP8I8ICZvZv4uFK7g/UXAHcSnw9oLnC5pOnA9cB5wXn+ezbv0bmBeGJxLjvmAxcBq4JhbuYTTwC9wE+DfX4CvFfSFGCqmS0P1j8MXCEpAlSZ2dMAZnbczDqCfV4ys2Yz6wXWAdXAYeA48H1JfwX07evcqPLE4lx2CHjYzOYFP3Vm9rUU+w02plKq6Rn6dCZ87gGKzayb+CR0TwHXAc+mF7JzmeGJxbns+C1wg6RZAJKmSzqT+N+5G4J9/hr4g5kdAg5Iel+w/pPA8mBOm2ZJ1wXnKJU0aaALBvPhTAkG/7wTmJfxu3JuGHx0Y+eywMxel/T/Ab+WNA44AdxGfKKr8yStAQ4R74cBuBn4bpA4EkcA/iTwPUlfD85x4yCXjQA/lzSBeGvnbzN8W84Ni49u7NwoknTEzMrDjsO5bPJHYc455zLKWyzOOecyylsszjnnMsoTi3POuYzyxOKccy6jPLE455zLKE8szjnnMur/Au8A/jlwGoAWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "metric='image_acc'\n",
        "\n",
        "plt.plot(history.history[metric], label=\"Train\")\n",
        "#plt.plot(history.history[f\"val_{metric}\"], label=\"Validation\")\n",
        "plt.title(metric)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(metric)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c068cc2-cbf8-4f99-bfe0-be05c093d79e",
      "metadata": {
        "id": "8c068cc2-cbf8-4f99-bfe0-be05c093d79e"
      },
      "outputs": [],
      "source": [
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71246165-b6a8-44c6-b20f-6c82cb555984",
      "metadata": {
        "id": "71246165-b6a8-44c6-b20f-6c82cb555984"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}